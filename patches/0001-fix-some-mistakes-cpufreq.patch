From 0afdd07357b108d17813b985e351d14baf6270a2 Mon Sep 17 00:00:00 2001
From: mostafa-z <mostafazarghami@gmail.com>
Date: Sun, 7 Jun 2015 05:10:18 +0430
Subject: [PATCH] fix some mistakes-cpufreq

---
 drivers/cpufreq/Kconfig                |  68 ++------------------------------------------------------------------
 drivers/cpufreq/Makefile               |   3 +--
 drivers/cpufreq/cpufreq_abyssplug.c    | 816 ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
 drivers/cpufreq/cpufreq_ondemandplus.c | 984 ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
 include/linux/cpufreq.h                |   9 +++------
 5 files changed, 6 insertions(+), 1874 deletions(-)
 delete mode 100644 drivers/cpufreq/cpufreq_abyssplug.c
 delete mode 100644 drivers/cpufreq/cpufreq_ondemandplus.c

diff --git a/drivers/cpufreq/Kconfig b/drivers/cpufreq/Kconfig
index 9833c16..dfab6f5 100755
--- a/drivers/cpufreq/Kconfig
+++ b/drivers/cpufreq/Kconfig
@@ -87,19 +87,6 @@ config CPU_FREQ_DEFAULT_GOV_SMARTASSV2
 		select CPU_FREQ_GOV_SMARTASSV2
 		help
 		Use the CPUFreq governor 'smartassV2' as default.
-		
-config CPU_FREQ_DEFAULT_GOV_ABYSSPLUG
-		bool "abyssplug"
-		select CPU_FREQ_GOV_ABYSSPLUG
-		select CPU_FREQ_GOV_PERFORMANCE
-	---help---
-		Use the CPUFreq governor 'abyssplug' as default. This allows you
-		to get a full dynamic frequency capable system with CPU
-		hotplug support by simply loading your cpufreq low-level
-		hardware driver. Be aware that not all cpufreq drivers
-		support the hotplug governor. If unsure have a look at
-		the help section of the driver. Fallback governor will be the
-		performance governor.
 
 config CPU_FREQ_DEFAULT_GOV_ABYSSPLUGV2
 		bool "abyssplugv2"
@@ -114,18 +101,6 @@ config CPU_FREQ_DEFAULT_GOV_ABYSSPLUGV2
 		the help section of the driver. Fallback governor will be the
 		performance governor.
 
-config CPU_FREQ_DEFAULT_GOV_ONDEMANDPLUS
-		bool "ondemandplus"
-		select CPU_FREQ_GOV_ONDEMANDPLUS
-		select CPU_FREQ_GOV_PERFORMANCE
-		---help---
-		Use the CPUFreq governor 'ondemandplus' as default. This allows
-		you to get a full dynamic frequency capable system by simply
-		loading your cpufreq low-level hardware driver.
-		Be aware that not all cpufreq drivers support the ondemandplus
-		governor. If unsure have a look at the help section of the
-		driver. Fallback governor will be the performance governor.
-
 config CPU_FREQ_DEFAULT_GOV_PERFORMANCE
 		bool "performance"
 		select CPU_FREQ_GOV_PERFORMANCE
@@ -226,26 +201,7 @@ config CPU_FREQ_GOV_INTELLIACTIVE
 		module will be called cpufreq_interactive.
 		For details, take a look at linux/Documentation/cpu-freq.
 		If in doubt, say N.
-
-config CPU_FREQ_GOV_ABYSSPLUG
-         tristate "'abyssplug' cpufreq governor"
-         depends on CPU_FREQ && NO_HZ && HOTPLUG_CPU
-         default y
-         ---help---
-         'abyssplug' - this driver mimics the frequency scaling behavior
-         in 'ondemand', but with several key differences. First is
-         that frequency transitions use the CPUFreq table directly,
-         instead of incrementing in a percentage of the maximum
-         available frequency. Second 'abyssplug' will offline auxillary
-         CPUs when the system is idle, and online those CPUs once the
-         system becomes busy again. This last feature is needed for
-         architectures which transition to low power states when only
-         the "master" CPU is online, or for thermally constrained
-         devices.
-         If you don't have one of these architectures or devices, use
-         'ondemand' instead.
-         If in doubt, say N.
-        
+       
 config CPU_FREQ_GOV_ABYSSPLUGV2
          tristate "'abyssplugv2' cpufreq governor"
          depends on CPU_FREQ && NO_HZ && HOTPLUG_CPU
@@ -265,26 +221,6 @@ config CPU_FREQ_GOV_ABYSSPLUGV2
          'ondemand' instead.
          If in doubt, say N.
         
-config CPU_FREQ_GOV_ONDEMANDPLUS
-         tristate "'ondemandplus' cpufreq policy governor"
-         select CPU_FREQ_TABLE
-         default y
-         ---help---
-         'ondemandplus' - This driver adds a dynamic cpufreq policy
-         governor. The governor does a periodic polling and
-         changes frequency based on the CPU utilization.
-         The support for this governor depends on CPU capability to
-         do fast frequency switching (i.e, very low latency frequency
-         transitions).
-        
-         To compile this driver as a module, choose M here: the
-         module will be called cpufreq_ondemandplus.
-        
-         For details, take a look at linux/Documentation/cpu-freq.
-        
-         If in doubt, say N.
-
-
 config CPU_FREQ_GOV_LAGFREE
 	tristate "'lagfree' cpufreq governor"
 	depends on CPU_FREQ
@@ -436,7 +372,7 @@ config SMARTASSV2_DOWN_RATE_US
 
 config SMARTASSV2_SLEEP_WAKEUP_FREQ
 	int "The frequency to set when waking up from sleep."
-	default 99999999
+	default 1190400
 	depends on CPU_FREQ_GOV_SMARTASSV2
 	help
 	The frequency to set when waking up from sleep.
diff --git a/drivers/cpufreq/Makefile b/drivers/cpufreq/Makefile
index 41e64de..eb0b716 100644
--- a/drivers/cpufreq/Makefile
+++ b/drivers/cpufreq/Makefile
@@ -9,15 +9,14 @@ obj-$(CONFIG_CPU_FREQ_GOV_POWERSAVE)	+= cpufreq_powersave.o
 obj-$(CONFIG_CPU_FREQ_GOV_USERSPACE)	+= cpufreq_userspace.o
 obj-$(CONFIG_CPU_FREQ_GOV_ONDEMAND)	+= cpufreq_ondemand.o
 obj-$(CONFIG_CPU_FREQ_GOV_CONSERVATIVE)	+= cpufreq_conservative.o
+obj-$(CONFIG_CPU_FREQ_GOV_ZZMOOVE)      += cpufreq_zzmoove.o
 obj-$(CONFIG_CPU_FREQ_GOV_INTERACTIVE)	+= cpufreq_interactive.o
 obj-$(CONFIG_CPU_FREQ_GOV_SMARTMAX) += cpufreq_smartmax.o
 obj-$(CONFIG_CPU_FREQ_GOV_INTELLIACTIVE)+= cpufreq_intelliactive.o
 obj-$(CONFIG_CPU_FREQ_GOV_LAGFREE) += cpufreq_lagfree.o
 obj-$(CONFIG_CPU_FREQ_GOV_LIONHEART) += cpufreq_lionheart.o
 obj-$(CONFIG_CPU_FREQ_GOV_SMARTASSV2) += cpufreq_smartass2.o
-obj-$(CONFIG_CPU_FREQ_GOV_ABYSSPLUG) += cpufreq_abyssplug.o
 obj-$(CONFIG_CPU_FREQ_GOV_ABYSSPLUGV2) += cpufreq_abyssplugv2.o
-obj-$(CONFIG_CPU_FREQ_GOV_ONDEMANDPLUS) += cpufreq_ondemandplus.o
 obj-$(CONFIG_CPU_FREQ_GOV_INTELLIMM) += cpufreq_intellimm.o
 
 # CPUfreq cross-arch helpers
diff --git a/drivers/cpufreq/cpufreq_abyssplug.c b/drivers/cpufreq/cpufreq_abyssplug.c
deleted file mode 100644
index 1aca95d..0000000
--- a/drivers/cpufreq/cpufreq_abyssplug.c
+++ /dev/null
@@ -1,816 +0,0 @@
-/*
- * CPUFreq AbyssPlug governor
- *
- *
- * Based on hotplug governor
- * Copyright (C) 2010 Texas Instruments, Inc.
- *   Mike Turquette <mturquette@ti.com>
- *   Santosh Shilimkar <santosh.shilimkar@ti.com>
- *
- * Based on ondemand governor
- * Copyright (C)  2001 Russell King
- *           (C)  2003 Venkatesh Pallipadi <venkatesh.pallipadi@intel.com>,
- *                     Jun Nakajima <jun.nakajima@intel.com>
- *
- * This program is free software; you can redistribute it and/or modify
- * it under the terms of the GNU General Public License version 2 as
- * published by the Free Software Foundation.
- */
-
-#include <linux/kernel.h>
-#include <linux/module.h>
-#include <linux/init.h>
-#include <linux/cpufreq.h>
-#include <linux/cpu.h>
-#include <linux/jiffies.h>
-#include <linux/kernel_stat.h>
-#include <linux/mutex.h>
-#include <linux/hrtimer.h>
-#include <linux/tick.h>
-#include <linux/ktime.h>
-#include <linux/sched.h>
-#include <linux/err.h>
-#include <linux/slab.h>
-
-/* greater than 95% avg load across online CPUs increases frequency */
-#define DEFAULT_UP_FREQ_MIN_LOAD			(95)
-
-/* Keep 10% of idle under the up threshold when decreasing the frequency */
-#define DEFAULT_FREQ_DOWN_DIFFERENTIAL			(1)
-
-/* less than 40% avg load across online CPUs decreases frequency */
-#define DEFAULT_DOWN_FREQ_MAX_LOAD			(40)
-
-/* default sampling period (uSec) is bogus; 10x ondemand's default for x86 */
-#define DEFAULT_SAMPLING_PERIOD				(50000)
-
-/* default number of sampling periods to average before hotplug-in decision */
-#define DEFAULT_HOTPLUG_IN_SAMPLING_PERIODS		(5)
-
-/* default number of sampling periods to average before hotplug-out decision */
-#define DEFAULT_HOTPLUG_OUT_SAMPLING_PERIODS		(20)
-
-static void do_dbs_timer(struct work_struct *work);
-static int cpufreq_governor_dbs(struct cpufreq_policy *policy,
-		unsigned int event);
-//static int hotplug_boost(struct cpufreq_policy *policy);
-
-#ifndef CONFIG_CPU_FREQ_DEFAULT_GOV_ABYSSPLUG
-static
-#endif
-struct cpufreq_governor cpufreq_gov_abyssplug = {
-       .name                   = "abyssplug",
-       .governor               = cpufreq_governor_dbs,
-       .owner                  = THIS_MODULE,
-};
-
-struct cpu_dbs_info_s {
-	cputime64_t prev_cpu_idle;
-	cputime64_t prev_cpu_wall;
-	cputime64_t prev_cpu_nice;
-	struct cpufreq_policy *cur_policy;
-	struct delayed_work work;
-	struct work_struct cpu_up_work;
-	struct work_struct cpu_down_work;
-	struct cpufreq_frequency_table *freq_table;
-	int cpu;
-	unsigned int boost_applied;
-	/*
-	 * percpu mutex that serializes governor limit change with
-	 * do_dbs_timer invocation. We do not want do_dbs_timer to run
-	 * when user is changing the governor or limits.
-	 */
-	struct mutex timer_mutex;
-};
-static DEFINE_PER_CPU(struct cpu_dbs_info_s, hp_cpu_dbs_info);
-
-static unsigned int dbs_enable;	/* number of CPUs using this policy */
-
-/*
- * dbs_mutex protects data in dbs_tuners_ins from concurrent changes on
- * different CPUs. It protects dbs_enable in governor start/stop.
- */
-static DEFINE_MUTEX(dbs_mutex);
-
-static struct workqueue_struct	*khotplug_wq;
-
-static struct dbs_tuners {
-	unsigned int sampling_rate;
-	unsigned int up_threshold;
-	unsigned int down_differential;
-	unsigned int down_threshold;
-	unsigned int hotplug_in_sampling_periods;
-	unsigned int hotplug_out_sampling_periods;
-	unsigned int hotplug_load_index;
-	unsigned int *hotplug_load_history;
-	unsigned int ignore_nice;
-	unsigned int io_is_busy;
-	unsigned int boost_timeout;
-} dbs_tuners_ins = {
-	.sampling_rate =		DEFAULT_SAMPLING_PERIOD,
-	.up_threshold =			DEFAULT_UP_FREQ_MIN_LOAD,
-	.down_differential =            DEFAULT_FREQ_DOWN_DIFFERENTIAL,
-	.down_threshold =		DEFAULT_DOWN_FREQ_MAX_LOAD,
-	.hotplug_in_sampling_periods =	DEFAULT_HOTPLUG_IN_SAMPLING_PERIODS,
-	.hotplug_out_sampling_periods =	DEFAULT_HOTPLUG_OUT_SAMPLING_PERIODS,
-	.hotplug_load_index =		0,
-	.ignore_nice =			0,
-	.io_is_busy =			0,
-	.boost_timeout = 0,
-};
-
-/*
- * A corner case exists when switching io_is_busy at run-time: comparing idle
- * times from a non-io_is_busy period to an io_is_busy period (or vice-versa)
- * will misrepresent the actual change in system idleness.  We ignore this
- * corner case: enabling io_is_busy might cause freq increase and disabling
- * might cause freq decrease, which probably matches the original intent.
- */
-static inline cputime64_t get_cpu_idle_time(unsigned int cpu, cputime64_t *wall)
-{
-        u64 idle_time;
-        u64 iowait_time;
-
-        /* cpufreq-abyssplug always assumes CONFIG_NO_HZ */
-        idle_time = get_cpu_idle_time_us(cpu, wall);
-
-	/* add time spent doing I/O to idle time */
-        if (dbs_tuners_ins.io_is_busy) {
-                iowait_time = get_cpu_iowait_time_us(cpu, wall);
-                /* cpufreq-abyssplug always assumes CONFIG_NO_HZ */
-                if (iowait_time != -1ULL && idle_time >= iowait_time)
-                        idle_time -= iowait_time;
-        }
-
-        return idle_time;
-}
-
-/************************** sysfs interface ************************/
-
-/* XXX look at global sysfs macros in cpufreq.h, can those be used here? */
-
-/* cpufreq_abyssplug Governor Tunables */
-#define show_one(file_name, object)					\
-static ssize_t show_##file_name						\
-(struct kobject *kobj, struct attribute *attr, char *buf)		\
-{									\
-	return sprintf(buf, "%u\n", dbs_tuners_ins.object);		\
-}
-show_one(sampling_rate, sampling_rate);
-show_one(up_threshold, up_threshold);
-show_one(down_differential, down_differential);
-show_one(down_threshold, down_threshold);
-show_one(hotplug_in_sampling_periods, hotplug_in_sampling_periods);
-show_one(hotplug_out_sampling_periods, hotplug_out_sampling_periods);
-show_one(ignore_nice_load, ignore_nice);
-show_one(io_is_busy, io_is_busy);
-show_one(boost_timeout, boost_timeout);
-
-static ssize_t store_boost_timeout(struct kobject *a, struct attribute *b,
-				   const char *buf, size_t count)
-{
-	unsigned int input;
-	int ret;
-	ret = sscanf(buf, "%u", &input);
-	if (ret != 1)
-		return -EINVAL;
-
-	mutex_lock(&dbs_mutex);
-	dbs_tuners_ins.boost_timeout = input;
-	mutex_unlock(&dbs_mutex);
-
-	return count;
-}
-
-static ssize_t store_sampling_rate(struct kobject *a, struct attribute *b,
-				   const char *buf, size_t count)
-{
-	unsigned int input;
-	int ret;
-	ret = sscanf(buf, "%u", &input);
-	if (ret != 1)
-		return -EINVAL;
-
-	mutex_lock(&dbs_mutex);
-	dbs_tuners_ins.sampling_rate = input;
-	mutex_unlock(&dbs_mutex);
-
-	return count;
-}
-
-static ssize_t store_up_threshold(struct kobject *a, struct attribute *b,
-				  const char *buf, size_t count)
-{
-	unsigned int input;
-	int ret;
-	ret = sscanf(buf, "%u", &input);
-
-	if (ret != 1 || input <= dbs_tuners_ins.down_threshold) {
-		return -EINVAL;
-	}
-
-	mutex_lock(&dbs_mutex);
-	dbs_tuners_ins.up_threshold = input;
-	mutex_unlock(&dbs_mutex);
-
-	return count;
-}
-
-static ssize_t store_down_differential(struct kobject *a, struct attribute *b,
-				  const char *buf, size_t count)
-{
-	unsigned int input;
-	int ret;
-	ret = sscanf(buf, "%u", &input);
-
-	if (ret != 1 || input >= dbs_tuners_ins.up_threshold)
-		return -EINVAL;
-
-	mutex_lock(&dbs_mutex);
-	dbs_tuners_ins.down_differential = input;
-	mutex_unlock(&dbs_mutex);
-
-	return count;
-}
-
-static ssize_t store_down_threshold(struct kobject *a, struct attribute *b,
-				  const char *buf, size_t count)
-{
-	unsigned int input;
-	int ret;
-	ret = sscanf(buf, "%u", &input);
-
-	if (ret != 1 || input >= dbs_tuners_ins.up_threshold) {
-		return -EINVAL;
-	}
-
-	mutex_lock(&dbs_mutex);
-	dbs_tuners_ins.down_threshold = input;
-	mutex_unlock(&dbs_mutex);
-
-	return count;
-}
-
-static ssize_t store_hotplug_in_sampling_periods(struct kobject *a,
-		struct attribute *b, const char *buf, size_t count)
-{
-	unsigned int input;
-	unsigned int *temp;
-	unsigned int max_windows;
-	int ret;
-	ret = sscanf(buf, "%u", &input);
-
-	if (ret != 1)
-		return -EINVAL;
-
-	/* already using this value, bail out */
-	if (input == dbs_tuners_ins.hotplug_in_sampling_periods)
-		return count;
-
-	mutex_lock(&dbs_mutex);
-	ret = count;
-	max_windows = max(dbs_tuners_ins.hotplug_in_sampling_periods,
-			dbs_tuners_ins.hotplug_out_sampling_periods);
-
-	/* no need to resize array */
-	if (input <= max_windows) {
-		dbs_tuners_ins.hotplug_in_sampling_periods = input;
-		goto out;
-	}
-
-	/* resize array */
-	temp = kmalloc((sizeof(unsigned int) * input), GFP_KERNEL);
-
-	if (!temp || IS_ERR(temp)) {
-		ret = -ENOMEM;
-		goto out;
-	}
-
-	memcpy(temp, dbs_tuners_ins.hotplug_load_history,
-			(max_windows * sizeof(unsigned int)));
-	kfree(dbs_tuners_ins.hotplug_load_history);
-
-	/* replace old buffer, old number of sampling periods & old index */
-	dbs_tuners_ins.hotplug_load_history = temp;
-	dbs_tuners_ins.hotplug_in_sampling_periods = input;
-	dbs_tuners_ins.hotplug_load_index = max_windows;
-out:
-	mutex_unlock(&dbs_mutex);
-
-	return ret;
-}
-
-static ssize_t store_hotplug_out_sampling_periods(struct kobject *a,
-		struct attribute *b, const char *buf, size_t count)
-{
-	unsigned int input;
-	unsigned int *temp;
-	unsigned int max_windows;
-	int ret;
-	ret = sscanf(buf, "%u", &input);
-
-	if (ret != 1)
-		return -EINVAL;
-
-	/* already using this value, bail out */
-	if (input == dbs_tuners_ins.hotplug_out_sampling_periods)
-		return count;
-
-	mutex_lock(&dbs_mutex);
-	ret = count;
-	max_windows = max(dbs_tuners_ins.hotplug_in_sampling_periods,
-			dbs_tuners_ins.hotplug_out_sampling_periods);
-
-	/* no need to resize array */
-	if (input <= max_windows) {
-		dbs_tuners_ins.hotplug_out_sampling_periods = input;
-		goto out;
-	}
-
-	/* resize array */
-	temp = kmalloc((sizeof(unsigned int) * input), GFP_KERNEL);
-
-	if (!temp || IS_ERR(temp)) {
-		ret = -ENOMEM;
-		goto out;
-	}
-
-	memcpy(temp, dbs_tuners_ins.hotplug_load_history,
-			(max_windows * sizeof(unsigned int)));
-	kfree(dbs_tuners_ins.hotplug_load_history);
-
-	/* replace old buffer, old number of sampling periods & old index */
-	dbs_tuners_ins.hotplug_load_history = temp;
-	dbs_tuners_ins.hotplug_out_sampling_periods = input;
-	dbs_tuners_ins.hotplug_load_index = max_windows;
-out:
-	mutex_unlock(&dbs_mutex);
-
-	return ret;
-}
-
-static ssize_t store_ignore_nice_load(struct kobject *a, struct attribute *b,
-				      const char *buf, size_t count)
-{
-	unsigned int input;
-	int ret;
-
-	unsigned int j;
-
-	ret = sscanf(buf, "%u", &input);
-	if (ret != 1)
-		return -EINVAL;
-
-	if (input > 1)
-		input = 1;
-
-	mutex_lock(&dbs_mutex);
-	if (input == dbs_tuners_ins.ignore_nice) { /* nothing to do */
-		mutex_unlock(&dbs_mutex);
-		return count;
-	}
-	dbs_tuners_ins.ignore_nice = input;
-
-	/* we need to re-evaluate prev_cpu_idle */
-	for_each_online_cpu(j) {
-		struct cpu_dbs_info_s *dbs_info;
-		dbs_info = &per_cpu(hp_cpu_dbs_info, j);
-		dbs_info->prev_cpu_idle = get_cpu_idle_time(j,
-						&dbs_info->prev_cpu_wall);
-		if (dbs_tuners_ins.ignore_nice)
-			dbs_info->prev_cpu_nice = kcpustat_cpu(j).cpustat[CPUTIME_NICE];
-
-	}
-	mutex_unlock(&dbs_mutex);
-
-	return count;
-}
-
-static ssize_t store_io_is_busy(struct kobject *a, struct attribute *b,
-				   const char *buf, size_t count)
-{
-	unsigned int input;
-	int ret;
-
-	ret = sscanf(buf, "%u", &input);
-	if (ret != 1)
-		return -EINVAL;
-
-	mutex_lock(&dbs_mutex);
-	dbs_tuners_ins.io_is_busy = !!input;
-	mutex_unlock(&dbs_mutex);
-
-	return count;
-}
-
-define_one_global_rw(sampling_rate);
-define_one_global_rw(up_threshold);
-define_one_global_rw(down_differential);
-define_one_global_rw(down_threshold);
-define_one_global_rw(hotplug_in_sampling_periods);
-define_one_global_rw(hotplug_out_sampling_periods);
-define_one_global_rw(ignore_nice_load);
-define_one_global_rw(io_is_busy);
-define_one_global_rw(boost_timeout);
-
-static struct attribute *dbs_attributes[] = {
-	&sampling_rate.attr,
-	&up_threshold.attr,
-	&down_differential.attr,
-	&down_threshold.attr,
-	&hotplug_in_sampling_periods.attr,
-	&hotplug_out_sampling_periods.attr,
-	&ignore_nice_load.attr,
-	&io_is_busy.attr,
-	&boost_timeout.attr,
-	NULL
-};
-
-static struct attribute_group dbs_attr_group = {
-	.attrs = dbs_attributes,
-	.name = "abyssplug",
-};
-
-/************************** sysfs end ************************/
-
-static void dbs_check_cpu(struct cpu_dbs_info_s *this_dbs_info)
-{
-	/* combined load of all enabled CPUs */
-	unsigned int total_load = 0;
-	/* single largest CPU load percentage*/
-	unsigned int max_load = 0;
-	/* largest CPU load in terms of frequency */
-	unsigned int max_load_freq = 0;
-	/* average load across all enabled CPUs */
-	unsigned int avg_load = 0;
-	/* average load across multiple sampling periods for hotplug events */
-	unsigned int hotplug_in_avg_load = 0;
-	unsigned int hotplug_out_avg_load = 0;
-	/* number of sampling periods averaged for hotplug decisions */
-	unsigned int periods;
-
-	struct cpufreq_policy *policy;
-	unsigned int i, j;
-
-	policy = this_dbs_info->cur_policy;
-
-	/*
-	 * cpu load accounting
-	 * get highest load, total load and average load across all CPUs
-	 */
-	for_each_cpu(j, policy->cpus) {
-		unsigned int load;
-		unsigned int idle_time, wall_time;
-		cputime64_t cur_wall_time, cur_idle_time;
-		struct cpu_dbs_info_s *j_dbs_info;
-
-		j_dbs_info = &per_cpu(hp_cpu_dbs_info, j);
-
-		/* update both cur_idle_time and cur_wall_time */
-		cur_idle_time = get_cpu_idle_time(j, &cur_wall_time);
-
-		/* how much wall time has passed since last iteration? */
-		wall_time = (unsigned int) cputime64_sub(cur_wall_time,
-				j_dbs_info->prev_cpu_wall);
-		j_dbs_info->prev_cpu_wall = cur_wall_time;
-
-		/* how much idle time has passed since last iteration? */
-		idle_time = (unsigned int) cputime64_sub(cur_idle_time,
-				j_dbs_info->prev_cpu_idle);
-		j_dbs_info->prev_cpu_idle = cur_idle_time;
-
-		if (unlikely(!wall_time || wall_time < idle_time))
-			continue;
-
-		/* load is the percentage of time not spent in idle */
-		load = 100 * (wall_time - idle_time) / wall_time;
-
-		/* keep track of combined load across all CPUs */
-		total_load += load;
-
-		/* keep track of highest single load across all CPUs */
-		if (load > max_load)
-			max_load = load;
-	}
-
-	/* use the max load in the OPP freq change policy */
-	max_load_freq = max_load * policy->cur;
-
-	/* calculate the average load across all related CPUs */
-	avg_load = total_load / num_online_cpus();
-
-	mutex_lock(&dbs_mutex);
-
-	/*
-	 * hotplug load accounting
-	 * average load over multiple sampling periods
-	 */
-
-	/* how many sampling periods do we use for hotplug decisions? */
-	periods = max(dbs_tuners_ins.hotplug_in_sampling_periods,
-			dbs_tuners_ins.hotplug_out_sampling_periods);
-
-	/* store avg_load in the circular buffer */
-	dbs_tuners_ins.hotplug_load_history[dbs_tuners_ins.hotplug_load_index]
-		= avg_load;
-
-	/* compute average load across in & out sampling periods */
-	for (i = 0, j = dbs_tuners_ins.hotplug_load_index;
-			i < periods; i++, j--) {
-		if (i < dbs_tuners_ins.hotplug_in_sampling_periods)
-			hotplug_in_avg_load +=
-				dbs_tuners_ins.hotplug_load_history[j];
-		if (i < dbs_tuners_ins.hotplug_out_sampling_periods)
-			hotplug_out_avg_load +=
-				dbs_tuners_ins.hotplug_load_history[j];
-
-		if (j == 0)
-			j = periods;
-	}
-
-	hotplug_in_avg_load = hotplug_in_avg_load /
-		dbs_tuners_ins.hotplug_in_sampling_periods;
-
-	hotplug_out_avg_load = hotplug_out_avg_load /
-		dbs_tuners_ins.hotplug_out_sampling_periods;
-
-	/* return to first element if we're at the circular buffer's end */
-	if (++dbs_tuners_ins.hotplug_load_index == periods)
-		dbs_tuners_ins.hotplug_load_index = 0;
-
-	/* check if auxiliary CPU is needed based on avg_load */
-	if (avg_load > dbs_tuners_ins.up_threshold) {
-		/* should we enable auxillary CPUs? */
-		if (num_online_cpus() < 2 && hotplug_in_avg_load >
-				dbs_tuners_ins.up_threshold) {
-			queue_work_on(this_dbs_info->cpu, khotplug_wq,
-					&this_dbs_info->cpu_up_work);
-			goto out;
-		}
-	}
-
-	/* check for frequency increase based on max_load */
-	if (max_load > dbs_tuners_ins.up_threshold) {
-		/* increase to highest frequency supported */
-		if (policy->cur < policy->max)
-			__cpufreq_driver_target(policy, policy->max,
-					CPUFREQ_RELATION_H);
-
-		goto out;
-	}
-
-	/* check for frequency decrease */
-	if (avg_load < dbs_tuners_ins.down_threshold) {
-		/* are we at the minimum frequency already? */
-		if (policy->cur <= policy->min) {
-			/* should we disable auxillary CPUs? */
-			if (num_online_cpus() > 1 && hotplug_out_avg_load <
-					dbs_tuners_ins.down_threshold) {
-				queue_work_on(this_dbs_info->cpu, khotplug_wq,
-					&this_dbs_info->cpu_down_work);
-			}
-			goto out;
-		}
-	}
-
-	/*
-	 * go down to the lowest frequency which can sustain the load by
-	 * keeping 30% of idle in order to not cross the up_threshold
-	 */
-	if ((max_load_freq <
-	    (dbs_tuners_ins.up_threshold - dbs_tuners_ins.down_differential) *
-	     policy->cur) && (policy->cur > policy->min)) {
-		unsigned int freq_next;
-		freq_next = max_load_freq /
-				(dbs_tuners_ins.up_threshold -
-				 dbs_tuners_ins.down_differential);
-
-		if (freq_next < policy->min)
-			freq_next = policy->min;
-
-		 __cpufreq_driver_target(policy, freq_next,
-					 CPUFREQ_RELATION_L);
-	}
-out:
-	mutex_unlock(&dbs_mutex);
-	return;
-}
-
-static void __cpuinit do_cpu_up(struct work_struct *work)
-{
-	cpu_up(1);
-}
-
-static void __cpuinit do_cpu_down(struct work_struct *work)
-{
-	cpu_down(1);
-}
-
-static void do_dbs_timer(struct work_struct *work)
-{
-	struct cpu_dbs_info_s *dbs_info =
-		container_of(work, struct cpu_dbs_info_s, work.work);
-	unsigned int cpu = dbs_info->cpu;
-	int delay = 0;
-
-	mutex_lock(&dbs_info->timer_mutex);
-	if (!dbs_info->boost_applied) {
-	    dbs_check_cpu(dbs_info);
-		/* We want all related CPUs to do sampling nearly on same jiffy */
-		delay = usecs_to_jiffies(dbs_tuners_ins.sampling_rate);
-	} else {
-		delay = usecs_to_jiffies(dbs_tuners_ins.boost_timeout);
-		dbs_info->boost_applied = 0;
-		if (num_online_cpus() < 2)
-			queue_work_on(cpu, khotplug_wq,
-						&dbs_info->cpu_up_work);
-	}
-	queue_delayed_work_on(cpu, khotplug_wq, &dbs_info->work, delay);
-	mutex_unlock(&dbs_info->timer_mutex);
-}
-
-static inline void dbs_timer_init(struct cpu_dbs_info_s *dbs_info)
-{
-	/* We want all related CPUs to do sampling nearly on same jiffy */
-	int delay = usecs_to_jiffies(dbs_tuners_ins.sampling_rate);
-	delay -= jiffies % delay;
-
-	INIT_DELAYED_WORK_DEFERRABLE(&dbs_info->work, do_dbs_timer);
-	if (!dbs_info->boost_applied)
-		delay = usecs_to_jiffies(dbs_tuners_ins.boost_timeout);
-	queue_delayed_work_on(dbs_info->cpu, khotplug_wq, &dbs_info->work,
-		delay);
-}
-
-static inline void dbs_timer_exit(struct cpu_dbs_info_s *dbs_info)
-{
-	cancel_delayed_work_sync(&dbs_info->work);
-}
-
-static int cpufreq_governor_dbs(struct cpufreq_policy *policy,
-				   unsigned int event)
-{
-	unsigned int cpu = policy->cpu;
-	struct cpu_dbs_info_s *this_dbs_info;
-	unsigned int i, j, max_periods;
-	int rc;
-
-	this_dbs_info = &per_cpu(hp_cpu_dbs_info, cpu);
-
-	switch (event) {
-	case CPUFREQ_GOV_START:
-		if ((!cpu_online(cpu)) || (!policy->cur))
-			return -EINVAL;
-
-		mutex_lock(&dbs_mutex);
-		dbs_enable++;
-		for_each_cpu(j, policy->cpus) {
-			struct cpu_dbs_info_s *j_dbs_info;
-			j_dbs_info = &per_cpu(hp_cpu_dbs_info, j);
-			j_dbs_info->cur_policy = policy;
-
-			j_dbs_info->prev_cpu_idle = get_cpu_idle_time(j,
-						&j_dbs_info->prev_cpu_wall);
-			if (dbs_tuners_ins.ignore_nice) {
-				j_dbs_info->prev_cpu_nice =
-						kcpustat_cpu(j).cpustat[CPUTIME_NICE];
-			}
-
-			max_periods = max(DEFAULT_HOTPLUG_IN_SAMPLING_PERIODS,
-					DEFAULT_HOTPLUG_OUT_SAMPLING_PERIODS);
-			dbs_tuners_ins.hotplug_load_history = kmalloc(
-					(sizeof(unsigned int) * max_periods),
-					GFP_KERNEL);
-			if (!dbs_tuners_ins.hotplug_load_history) {
-				WARN_ON(1);
-				return -ENOMEM;
-			}
-			for (i = 0; i < max_periods; i++)
-				dbs_tuners_ins.hotplug_load_history[i] = 50;
-		}
-		this_dbs_info->cpu = cpu;
-		this_dbs_info->freq_table = cpufreq_frequency_get_table(cpu);
-		/*
-		 * Start the timerschedule work, when this governor
-		 * is used for first time
-		 */
-		if (dbs_enable == 1) {
-			rc = sysfs_create_group(cpufreq_global_kobject,
-						&dbs_attr_group);
-			if (rc) {
-				mutex_unlock(&dbs_mutex);
-				return rc;
-			}
-		}
-		if (!dbs_tuners_ins.boost_timeout)
-			dbs_tuners_ins.boost_timeout =  dbs_tuners_ins.sampling_rate * 30;
-		mutex_unlock(&dbs_mutex);
-
-		mutex_init(&this_dbs_info->timer_mutex);
-		dbs_timer_init(this_dbs_info);
-		break;
-
-	case CPUFREQ_GOV_STOP:
-		dbs_timer_exit(this_dbs_info);
-
-		mutex_lock(&dbs_mutex);
-		mutex_destroy(&this_dbs_info->timer_mutex);
-		dbs_enable--;
-		mutex_unlock(&dbs_mutex);
-		if (!dbs_enable)
-			sysfs_remove_group(cpufreq_global_kobject,
-					   &dbs_attr_group);
-		kfree(dbs_tuners_ins.hotplug_load_history);
-		/*
-		 * XXX BIG CAVEAT: Stopping the governor with CPU1 offline
-		 * will result in it remaining offline until the user onlines
-		 * it again.  It is up to the user to do this (for now).
-		 */
-		break;
-
-	case CPUFREQ_GOV_LIMITS:
-		mutex_lock(&this_dbs_info->timer_mutex);
-		if (policy->max < this_dbs_info->cur_policy->cur)
-			__cpufreq_driver_target(this_dbs_info->cur_policy,
-				policy->max, CPUFREQ_RELATION_H);
-		else if (policy->min > this_dbs_info->cur_policy->cur)
-			__cpufreq_driver_target(this_dbs_info->cur_policy,
-				policy->min, CPUFREQ_RELATION_L);
-		mutex_unlock(&this_dbs_info->timer_mutex);
-		break;
-	}
-	return 0;
-}
-
-#if 0
-static int hotplug_boost(struct cpufreq_policy *policy)
-{
-	unsigned int cpu = policy->cpu;
-	struct cpu_dbs_info_s *this_dbs_info;
-
-	this_dbs_info = &per_cpu(hp_cpu_dbs_info, cpu);
-
-#if 0
-	/* Already at max? */
-	if (policy->cur == policy->max)
-		return;
-#endif
-
-	mutex_lock(&this_dbs_info->timer_mutex);
-	this_dbs_info->boost_applied = 1;
-	__cpufreq_driver_target(policy, policy->max,
-		CPUFREQ_RELATION_H);
-	mutex_unlock(&this_dbs_info->timer_mutex);
-
-	return 0;
-}
-#endif
-
-static int __init cpufreq_gov_dbs_init(void)
-{
-	int err;
-	cputime64_t wall;
-	u64 idle_time;
-	int cpu = get_cpu();
-	struct cpu_dbs_info_s *dbs_info = &per_cpu(hp_cpu_dbs_info, 0);
-
-	INIT_WORK(&dbs_info->cpu_up_work, do_cpu_up);
-	INIT_WORK(&dbs_info->cpu_down_work, do_cpu_down);
-
-	idle_time = get_cpu_idle_time_us(cpu, &wall);
-	put_cpu();
-	if (idle_time != -1ULL) {
-		dbs_tuners_ins.up_threshold = DEFAULT_UP_FREQ_MIN_LOAD;
-	} else {
-		pr_err("cpufreq-abyssplug: %s: assumes CONFIG_NO_HZ\n",
-				__func__);
-		return -EINVAL;
-	}
-
-	khotplug_wq = create_workqueue("khotplug");
-	if (!khotplug_wq) {
-		pr_err("Creation of khotplug failed\n");
-		return -EFAULT;
-	}
-	err = cpufreq_register_governor(&cpufreq_gov_abyssplug);
-	if (err)
-		destroy_workqueue(khotplug_wq);
-
-	return err;
-}
-
-static void __exit cpufreq_gov_dbs_exit(void)
-{
-	cpufreq_unregister_governor(&cpufreq_gov_abyssplug);
-	destroy_workqueue(khotplug_wq);
-}
-
-MODULE_DESCRIPTION("'cpufreq_abyssplug' - cpufreq governor for dynamic frequency scaling and CPU hotplug");
-MODULE_LICENSE("GPL");
-
-#ifdef CONFIG_CPU_FREQ_DEFAULT_GOV_ABYSSPLUG
-fs_initcall(cpufreq_gov_dbs_init);
-#else
-module_init(cpufreq_gov_dbs_init);
-#endif
-module_exit(cpufreq_gov_dbs_exit);
diff --git a/drivers/cpufreq/cpufreq_ondemandplus.c b/drivers/cpufreq/cpufreq_ondemandplus.c
deleted file mode 100644
index 613f960..0000000
--- a/drivers/cpufreq/cpufreq_ondemandplus.c
+++ /dev/null
@@ -1,984 +0,0 @@
-/*
- * drivers/cpufreq/cpufreq_ondemandplus.c
- * Copyright (C) 2013 Boy Petersen
- *
- * This program is free software; you can redistribute it and/or modify
- * it under the terms of the GNU General Public License version 2 as
- * published by the Free Software Foundation.
- *
- *
- * based upon:
- *
- *
- *         drivers/cpufreq/cpufreq_interactive.c
- *
- *         Copyright (C) 2010 Google, Inc.
- *
- *         This software is licensed under the terms of the GNU General Public
- *         License version 2, as published by the Free Software Foundation, and
- *         may be copied, distributed, and modified under those terms.
- *
- *         This program is distributed in the hope that it will be useful,
- *         but WITHOUT ANY WARRANTY; without even the implied warranty of
- *         MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
- *         GNU General Public License for more details.
- *
- *         Author: Mike Chan (mike@android.com)
- *
- *
- * and:
- *
- *         drivers/cpufreq/cpufreq_ondemand.c
- *
- *         Copyright (C)  2001 Russell King
- *                     (C)  2003 Venkatesh Pallipadi <venkatesh.pallipadi@intel.com>.
- *                                 Jun Nakajima <jun.nakajima@intel.com>
- *
- *         This program is free software; you can redistribute it and/or modify
- *         it under the terms of the GNU General Public License version 2 as
- *         published by the Free Software Foundation.
- */
-
-#include <linux/cpu.h>
-#include <linux/cpumask.h>
-#include <linux/cpufreq.h>
-#include <linux/mutex.h>
-#include <linux/sched.h>
-#include <linux/tick.h>
-#include <linux/time.h>
-#include <linux/timer.h>
-#include <linux/workqueue.h>
-#include <linux/kthread.h>
-#include <linux/mutex.h>
-#include <linux/slab.h>
-#include <linux/kernel_stat.h>
-#include <asm/cputime.h>
-#include <linux/module.h>
-
-#define CREATE_TRACE_POINTS
-#include <trace/events/cpufreq_ondemandplus.h>
-
-static atomic_t active_count = ATOMIC_INIT(0);
-
-struct cpufreq_ondemandplus_cpuinfo {
-        struct timer_list cpu_timer;
-        int timer_idlecancel;
-        u64 time_in_idle;
-        u64 idle_exit_time;
-        u64 timer_run_time;
-        int idling;
-        u64 target_set_time;
-        u64 target_set_time_in_idle;
-        struct cpufreq_policy *policy;
-        struct cpufreq_frequency_table *freq_table;
-        unsigned int target_freq;
-        int governor_enabled;
-};
-
-static DEFINE_PER_CPU(struct cpufreq_ondemandplus_cpuinfo, cpuinfo);
-
-/* realtime thread handles frequency scaling */
-static struct task_struct *speedchange_task;
-static cpumask_t speedchange_cpumask;
-static spinlock_t speedchange_cpumask_lock;
-
-/*
- * Tunables start
- */
-
-#define DEFAULT_TIMER_RATE (20 * USEC_PER_MSEC)
-static unsigned long timer_rate;
-
-#define DEFAULT_UP_THRESHOLD 70
-static unsigned long up_threshold;
-
-#define DEFAULT_DOWN_DIFFERENTIAL 20
-static unsigned long down_differential;
-
-#define DEFAULT_MIN_FREQ 300000
-static u64 allowed_min;
-
-#define DEFAULT_MAX_FREQ 2265600
-static u64 allowed_max;
-
-#define DEFAULT_INTER_HIFREQ 1728000
-static u64 inter_hifreq;
-
-#define DEFAULT_INTER_LOFREQ 300000
-static u64 inter_lofreq;
-
-#define SUSPEND_FREQ 300000
-static u64 suspend_frequency;
-
-#define DEFAULT_INTER_STAYCYCLES 2
-static unsigned long inter_staycycles;
-
-#define DEFAULT_STAYCYCLES_RESETFREQ 652800
-static u64 staycycles_resetfreq;
-
-#define DEFAULT_IO_IS_BUSY 0
-static unsigned int io_is_busy;
-
-/*
- * Tunables end
- */
-
-static int cpufreq_governor_ondemandplus(struct cpufreq_policy *policy,
-                unsigned int event);
-
-#ifndef CONFIG_CPU_FREQ_DEFAULT_GOV_ONDEMANDPLUS
-static
-#endif
-struct cpufreq_governor cpufreq_gov_ondemandplus = {
-        .name = "ondemandplus",
-        .governor = cpufreq_governor_ondemandplus,
-        .max_transition_latency = 10000000,
-        .owner = THIS_MODULE,
-};
-
-static inline cputime64_t get_cpu_idle_time_jiffy(unsigned int cpu,
-                                                  cputime64_t *wall)
-{
-        u64 idle_time;
-        u64 cur_wall_time;
-        u64 busy_time;
-
-        cur_wall_time = jiffies64_to_cputime64(get_jiffies_64());
-
-        busy_time  = kcpustat_cpu(cpu).cpustat[CPUTIME_USER];
-        busy_time += kcpustat_cpu(cpu).cpustat[CPUTIME_SYSTEM];
-        busy_time += kcpustat_cpu(cpu).cpustat[CPUTIME_IRQ];
-        busy_time += kcpustat_cpu(cpu).cpustat[CPUTIME_SOFTIRQ];
-        busy_time += kcpustat_cpu(cpu).cpustat[CPUTIME_STEAL];
-        busy_time += kcpustat_cpu(cpu).cpustat[CPUTIME_NICE];
-
-        idle_time = cur_wall_time - busy_time;
-        if (wall)
-                *wall = jiffies_to_usecs(cur_wall_time);
-
-        return jiffies_to_usecs(idle_time);
-}
-
-static inline cputime64_t get_cpu_idle_time(unsigned int cpu,
-                                            cputime64_t *wall)
-{
-        u64 idle_time = get_cpu_idle_time_us(cpu, wall);
-
-        if (idle_time == -1ULL)
-                idle_time = get_cpu_idle_time_jiffy(cpu, wall);        
-        else if (io_is_busy == 2)
-                idle_time += (get_cpu_iowait_time_us(cpu, wall) / 2);
-        else if (!io_is_busy)
-                idle_time += get_cpu_iowait_time_us(cpu, wall);
-
-        return idle_time;
-}
-
-static void cpufreq_ondemandplus_timer(unsigned long data)
-{
-        unsigned int delta_idle;
-        unsigned int delta_time;
-        int cpu_load;
-        unsigned int load_freq;
-        int load_since_change;
-        u64 time_in_idle;
-        u64 idle_exit_time;
-        struct cpufreq_ondemandplus_cpuinfo *pcpu =
-                &per_cpu(cpuinfo, data);
-        u64 now_idle;
-        unsigned int new_freq;
-        unsigned int index;
-        static unsigned int stay_counter;
-        unsigned long flags;
-
-        smp_rmb();
-
-        if (!pcpu->governor_enabled)
-                goto exit;
-
-        /*
-         * Once pcpu->timer_run_time is updated to >= pcpu->idle_exit_time,
-         * this lets idle exit know the current idle time sample has
-         * been processed, and idle exit can generate a new sample and
-         * re-arm the timer.  This prevents a concurrent idle
-         * exit on that CPU from writing a new set of info at the same time
-         * the timer function runs (the timer function can't use that info
-         * until more time passes).
-         */
-
-        time_in_idle = pcpu->time_in_idle;
-        idle_exit_time = pcpu->idle_exit_time;
-        now_idle = get_cpu_idle_time(data, &pcpu->timer_run_time);
-        smp_wmb();
-
-        /* If we raced with cancelling a timer, skip. */
-        if (!idle_exit_time)
-                goto exit;
-
-        delta_idle = (unsigned int) (now_idle - time_in_idle);
-        delta_time = (unsigned int) (pcpu->timer_run_time - idle_exit_time);
-
-        /*
-         * If timer ran less than 1ms after short-term sample started, retry.
-         */
-        if (delta_time < 1000)
-                goto rearm;
-
-        if (delta_idle > delta_time)
-                cpu_load = 0;
-        else
-                cpu_load = 100 * (delta_time - delta_idle) / delta_time;
-
-        delta_idle = (unsigned int) (now_idle -        pcpu->target_set_time_in_idle);
-        delta_time = (unsigned int) (pcpu->timer_run_time - pcpu->target_set_time);
-
-        if ((delta_time == 0) || (delta_idle > delta_time))
-                load_since_change = 0;
-        else
-                load_since_change =
-                        100 * (delta_time - delta_idle) / delta_time;
-
-        /*
-         * If short-term load (since last idle timer started or
-         * timer function re-armed itself) is higher than long-term 
-         * load (since last frequency change), use short-term load
-         * to be able to scale up quickly.
-         * When long-term load is higher than short-term load, 
-         * use the average of short-term load and long-term load
-         * (instead of just long-term load) to be able to scale
-         * down faster, with the long-term load being able to delay 
-         * down scaling a little to maintain responsiveness.
-         */
-        if (load_since_change > cpu_load) {
-                cpu_load = (cpu_load + load_since_change) / 2;
-        }
-
-        load_freq = cpu_load * pcpu->target_freq;
-
-        new_freq = pcpu->target_freq;
-
-        /* suspended scaling behavior */
-        if (allowed_max == suspend_frequency) {
-                if (stay_counter) {
-                        stay_counter = 0;
-                }
-                
-                /* Check for frequency increase */
-                if (load_freq > up_threshold * pcpu->target_freq) {
-                        /* if we are already at full speed then break out early */
-                        if (pcpu->target_freq < suspend_frequency) {
-                                
-                                new_freq = pcpu->target_freq + pcpu->policy->max / 10;
-
-                                if (new_freq > suspend_frequency) {
-                                        new_freq = suspend_frequency;
-                                }
-                                
-                                cpufreq_frequency_table_target(pcpu->policy, pcpu->freq_table, new_freq,
-                                                CPUFREQ_RELATION_L, &index);
-                                
-                                new_freq = pcpu->freq_table[index].frequency;
-                        }
-
-                /* Check for frequency decrease */
-
-                /*
-                * The optimal frequency is the frequency that is the lowest that
-                * can support the current CPU usage without triggering the up
-                * policy. To be safe, we focus 10 points under the threshold.
-                */
-                } else if (load_freq < (up_threshold - down_differential) *
-                                pcpu->target_freq) {
-                        /* if we are already at full speed then break out early */
-                        if (pcpu->target_freq != pcpu->policy->min) {
-
-                                new_freq = pcpu->target_freq - pcpu->policy->max / 10;
-
-                                if (new_freq < pcpu->policy->min) {
-                                        new_freq = pcpu->policy->min;
-                                }
-                        
-                                cpufreq_frequency_table_target(pcpu->policy, pcpu->freq_table, new_freq,
-                                                CPUFREQ_RELATION_H, &index);
-                                
-                                new_freq = pcpu->freq_table[index].frequency;
-                        }
-                }
-        /* screen-on scaling behavior */
-        } else {
-                /* Check for frequency increase */
-                if (load_freq > up_threshold * pcpu->target_freq) {
-                        /* if we are already at full speed then break out early */
-                        if (pcpu->target_freq < pcpu->policy->max) {
-
-                                if (stay_counter == 0 && inter_staycycles != 0) {
-                                        new_freq = inter_lofreq;
-                                        stay_counter++;
-                                } else if (stay_counter == 1 && inter_staycycles != 1) {
-                                        new_freq = inter_hifreq;
-                                        stay_counter++;
-                                } else if (stay_counter < inter_staycycles) {
-                                        stay_counter++;
-                                        goto rearm;
-                                } else {
-                                        new_freq = pcpu->policy->max;
-                                }
-                        }
-                }
-
-                /* Check for frequency decrease */
-
-                /*
-                * The optimal frequency is the frequency that is the lowest that
-                * can support the current CPU usage without triggering the up
-                * policy. To be safe, we focus 10 points under the threshold.
-                */
-                if (load_freq < (up_threshold - down_differential) *
-                                pcpu->target_freq) {
-                        
-                        if (pcpu->target_freq != allowed_min) {
-                                new_freq = load_freq /
-                                                (up_threshold - down_differential);
-
-                                if (new_freq <= staycycles_resetfreq) {
-                                        stay_counter = 0;
-                                }
-
-                                if (new_freq < allowed_min) {
-                                        new_freq = allowed_min;
-                                }
-                        }
-                } else if (pcpu->target_freq == pcpu->policy->max && 
-                                load_freq < (up_threshold - down_differential / 2) * 
-                                pcpu->target_freq) {
-                        new_freq = load_freq / (up_threshold - down_differential * 2 / 3);
-                }
-
-        }
-
-        if (cpufreq_frequency_table_target(pcpu->policy, pcpu->freq_table,
-                                           new_freq, CPUFREQ_RELATION_H,
-                                           &index)) {
-                pr_warn_once("timer %d: cpufreq_frequency_table_target error\n",
-                             (int) data);
-                goto rearm;
-        }
-
-        new_freq = pcpu->freq_table[index].frequency;        
-
-        if (pcpu->target_freq == new_freq) {
-                trace_cpufreq_ondemandplus_already(data, cpu_load,
-                                                  pcpu->target_freq, new_freq);
-                goto rearm_if_notmax;
-        }
-
-        trace_cpufreq_ondemandplus_target(data, cpu_load, pcpu->target_freq,
-                                         new_freq);
-        pcpu->target_set_time_in_idle = now_idle;
-        pcpu->target_set_time = pcpu->timer_run_time;
-
-        pcpu->target_freq = new_freq;
-        spin_lock_irqsave(&speedchange_cpumask_lock, flags);
-        cpumask_set_cpu(data, &speedchange_cpumask);
-        spin_unlock_irqrestore(&speedchange_cpumask_lock, flags);
-        wake_up_process(speedchange_task);
-
-rearm_if_notmax:
-        /*
-         * Already set max speed and don't see a need to change that,
-         * wait until next idle to re-evaluate, don't need timer.
-         */
-        if (pcpu->target_freq == pcpu->policy->max)
-                goto exit;
-
-rearm:
-        if (!timer_pending(&pcpu->cpu_timer)) {
-                /*
-                 * If already at min: if that CPU is idle, don't set timer.
-                 * Else cancel the timer if that CPU goes idle.  We don't
-                 * need to re-evaluate speed until the next idle exit.
-                 */
-                 
-                unsigned int cur_min_policy;
-                if (allowed_max == suspend_frequency) {
-                        cur_min_policy = pcpu->policy->min;
-                } else {
-                        cur_min_policy = allowed_min;
-                }
-                
-                if (pcpu->target_freq == cur_min_policy) {
-                        smp_rmb();
-
-                        if (pcpu->idling)
-                                goto exit;
-
-                        pcpu->timer_idlecancel = 1;
-                }
-
-                pcpu->time_in_idle = get_cpu_idle_time(
-                        data, &pcpu->idle_exit_time);
-                mod_timer(&pcpu->cpu_timer,
-                        jiffies + usecs_to_jiffies(timer_rate));
-        }
-
-exit:
-        return;
-}
-
-static void cpufreq_ondemandplus_idle_start(void)
-{
-        struct cpufreq_ondemandplus_cpuinfo *pcpu =
-                &per_cpu(cpuinfo, smp_processor_id());
-        int pending;
-
-        if (!pcpu->governor_enabled)
-                return;
-
-        pcpu->idling = 1;
-        smp_wmb();
-        pending = timer_pending(&pcpu->cpu_timer);
-
-        if (pcpu->target_freq != pcpu->policy->min) {
-#ifdef CONFIG_SMP
-                /*
-                 * Entering idle while not at lowest speed.  On some
-                 * platforms this can hold the other CPU(s) at that speed
-                 * even though the CPU is idle. Set a timer to re-evaluate
-                 * speed so this idle CPU doesn't hold the other CPUs above
-                 * min indefinitely.  This should probably be a quirk of
-                 * the CPUFreq driver.
-                 */
-                if (!pending) {
-                        pcpu->time_in_idle = get_cpu_idle_time(
-                                smp_processor_id(), &pcpu->idle_exit_time);
-                        pcpu->timer_idlecancel = 0;
-                        mod_timer(&pcpu->cpu_timer,
-                                  jiffies + usecs_to_jiffies(timer_rate));
-                }
-#endif
-        } else {
-                /*
-                 * If at min speed and entering idle after load has
-                 * already been evaluated, and a timer has been set just in
-                 * case the CPU suddenly goes busy, cancel that timer.  The
-                 * CPU didn't go busy; we'll recheck things upon idle exit.
-                 */
-                if (pending && pcpu->timer_idlecancel) {
-                        del_timer(&pcpu->cpu_timer);
-                        /*
-                         * Ensure last timer run time is after current idle
-                         * sample start time, so next idle exit will always
-                         * start a new idle sampling period.
-                         */
-                        pcpu->idle_exit_time = 0;
-                        pcpu->timer_idlecancel = 0;
-                }
-        }
-
-}
-
-static void cpufreq_ondemandplus_idle_end(void)
-{
-        struct cpufreq_ondemandplus_cpuinfo *pcpu =
-                &per_cpu(cpuinfo, smp_processor_id());
-
-        pcpu->idling = 0;
-        smp_wmb();
-
-        /*
-         * Arm the timer for 1-2 ticks later if not already, and if the timer
-         * function has already processed the previous load sampling
-         * interval.  (If the timer is not pending but has not processed
-         * the previous interval, it is probably racing with us on another
-         * CPU.  Let it compute load based on the previous sample and then
-         * re-arm the timer for another interval when it's done, rather
-         * than updating the interval start time to be "now", which doesn't
-         * give the timer function enough time to make a decision on this
-         * run.)
-         */
-        if (timer_pending(&pcpu->cpu_timer) == 0 &&
-            pcpu->timer_run_time >= pcpu->idle_exit_time &&
-            pcpu->governor_enabled) {
-                pcpu->time_in_idle =
-                        get_cpu_idle_time(smp_processor_id(),
-                                             &pcpu->idle_exit_time);
-                pcpu->timer_idlecancel = 0;
-                mod_timer(&pcpu->cpu_timer,
-                          jiffies + usecs_to_jiffies(timer_rate));
-        }
-
-}
-
-static int cpufreq_ondemandplus_speedchange_task(void *data)
-{
-        unsigned int cpu;
-        cpumask_t tmp_mask;
-        unsigned long flags;
-        struct cpufreq_ondemandplus_cpuinfo *pcpu;
-
-        while (1) {
-                set_current_state(TASK_INTERRUPTIBLE);
-                spin_lock_irqsave(&speedchange_cpumask_lock, flags);
-
-                if (cpumask_empty(&speedchange_cpumask)) {
-                        spin_unlock_irqrestore(&speedchange_cpumask_lock,
-                                                flags);
-                        schedule();
-
-                        if (kthread_should_stop())
-                                break;
-
-                        spin_lock_irqsave(&speedchange_cpumask_lock, flags);
-                }
-
-                set_current_state(TASK_RUNNING);
-                tmp_mask = speedchange_cpumask;
-                cpumask_clear(&speedchange_cpumask);
-                spin_unlock_irqrestore(&speedchange_cpumask_lock, flags);
-
-                for_each_cpu(cpu, &tmp_mask) {
-                        unsigned int j;
-                        unsigned int max_freq = 0;
-
-                        pcpu = &per_cpu(cpuinfo, cpu);
-                        smp_rmb();
-
-                        if (!pcpu->governor_enabled)
-                                continue;
-
-                        for_each_cpu(j, pcpu->policy->cpus) {
-                                struct cpufreq_ondemandplus_cpuinfo *pjcpu =
-                                        &per_cpu(cpuinfo, j);
-
-                                if (pjcpu->target_freq > max_freq)
-                                        max_freq = pjcpu->target_freq;
-                        }
-
-                        if (max_freq != pcpu->policy->cur)
-                                __cpufreq_driver_target(pcpu->policy,
-                                                        max_freq,
-                                                        CPUFREQ_RELATION_H);
-                        trace_cpufreq_ondemandplus_setspeed(cpu,
-                                                pcpu->target_freq,
-                                                pcpu->policy->cur);
-                }
-        }
-
-        return 0;
-}
-
-static ssize_t show_timer_rate(struct kobject *kobj,
-                        struct attribute *attr, char *buf)
-{
-        return sprintf(buf, "%lu\n", timer_rate);
-}
-
-static ssize_t store_timer_rate(struct kobject *kobj,
-                        struct attribute *attr, const char *buf, size_t count)
-{
-        int ret;
-        unsigned long val;
-
-        ret = strict_strtoul(buf, 0, &val);
-        if (ret < 0)
-                return ret;
-
-        timer_rate = val;
-        return count;
-}
-
-static struct global_attr timer_rate_attr = __ATTR(timer_rate, 0644,
-                show_timer_rate, store_timer_rate);
-        
-static ssize_t show_up_threshold(struct kobject *kobj,
-                        struct attribute *attr, char *buf)
-{
-        return sprintf(buf, "%lu\n", up_threshold);
-}
-
-static ssize_t store_up_threshold(struct kobject *kobj,
-                        struct attribute *attr, const char *buf, size_t count)
-{
-        int ret;
-        unsigned long val;
-
-        ret = strict_strtoul(buf, 0, &val);
-        if (ret < 0)
-                return ret;
-                
-        if (val > 100)
-                val = 100;
-
-        if (val < 1)
-                val = 1;
-                
-        up_threshold = val;
-        return count;
-}
-
-static struct global_attr up_threshold_attr = __ATTR(up_threshold, 0644,
-                show_up_threshold, store_up_threshold);
-                
-static ssize_t show_down_differential(struct kobject *kobj,
-                        struct attribute *attr, char *buf)
-{
-        return sprintf(buf, "%lu\n", down_differential);
-}
-
-static ssize_t store_down_differential(struct kobject *kobj,
-                        struct attribute *attr, const char *buf, size_t count)
-{
-        int ret;
-        unsigned long val;
-
-        ret = strict_strtoul(buf, 0, &val);
-        if (ret < 0)
-                return ret;
-
-        if (val > 100)
-                val = 100;
-
-        down_differential = val;
-        return count;
-}
-
-static struct global_attr down_differential_attr = __ATTR(down_differential, 0644,
-                show_down_differential, store_down_differential);
-                
-static ssize_t show_inter_hifreq(struct kobject *kobj,
-                                 struct attribute *attr, char *buf)
-{
-        return sprintf(buf, "%llu\n", inter_hifreq);
-}
-
-static ssize_t store_inter_hifreq(struct kobject *kobj,
-                                  struct attribute *attr, const char *buf,
-                                  size_t count)
-{
-        int ret;
-        u64 val;
-        struct cpufreq_ondemandplus_cpuinfo *pcpu =
-                &per_cpu(cpuinfo, smp_processor_id());
-        unsigned int index;
-
-        ret = strict_strtoull(buf, 0, &val);
-        if (ret < 0)
-                return ret;
-        
-        index = 0;
-        cpufreq_frequency_table_target(pcpu->policy, pcpu->freq_table, val,
-                CPUFREQ_RELATION_L, &index);
-        val = pcpu->freq_table[index].frequency;
-
-        if (val > pcpu->policy->max)
-                val = pcpu->policy->max;
-
-        if (val < allowed_min)
-                val = allowed_min;
-
-        inter_hifreq = val;
-        return count;
-}
-
-static struct global_attr inter_hifreq_attr = __ATTR(inter_hifreq, 0644,
-                show_inter_hifreq, store_inter_hifreq);
-                
-static ssize_t show_inter_lofreq(struct kobject *kobj,
-                                 struct attribute *attr, char *buf)
-{
-        return sprintf(buf, "%llu\n", inter_lofreq);
-}
-
-static ssize_t store_inter_lofreq(struct kobject *kobj,
-                                  struct attribute *attr, const char *buf,
-                                  size_t count)
-{
-        int ret;
-        u64 val;
-        struct cpufreq_ondemandplus_cpuinfo *pcpu =
-                &per_cpu(cpuinfo, smp_processor_id());
-        unsigned int index;
-
-        ret = strict_strtoull(buf, 0, &val);
-        if (ret < 0)
-                return ret;
-
-        index = 0;
-        cpufreq_frequency_table_target(pcpu->policy, pcpu->freq_table, val,
-                        CPUFREQ_RELATION_H, &index);
-        val = pcpu->freq_table[index].frequency;
-
-        if (val > pcpu->policy->max)
-                val = pcpu->policy->max;
-
-        if (val < allowed_min)
-                val = allowed_min;
-        
-        inter_lofreq = val;
-        return count;
-}
-
-static struct global_attr inter_lofreq_attr = __ATTR(inter_lofreq, 0644,
-                show_inter_lofreq, store_inter_lofreq);
-
-static ssize_t show_inter_staycycles(struct kobject *kobj,
-                                        struct attribute *attr, char *buf)
-{
-        return sprintf(buf, "%lu\n", inter_staycycles);
-}
-
-static ssize_t store_inter_staycycles(struct kobject *kobj,
-                        struct attribute *attr, const char *buf, size_t count)
-{
-        int ret;
-        unsigned long val;
-
-        ret = strict_strtoul(buf, 0, &val);
-        if (ret < 0)
-                return ret;
-                
-        if (val > 10)
-                val = 10;
-                
-        inter_staycycles = val;
-        return count;
-}
-
-static struct global_attr inter_staycycles_attr = __ATTR(inter_staycycles, 0644,
-                show_inter_staycycles, store_inter_staycycles);
-                
-static ssize_t show_staycycles_resetfreq(struct kobject *kobj,
-                                 struct attribute *attr, char *buf)
-{
-        return sprintf(buf, "%llu\n", staycycles_resetfreq);
-}
-
-static ssize_t store_staycycles_resetfreq(struct kobject *kobj,
-                                  struct attribute *attr, const char *buf,
-                                  size_t count)
-{
-        int ret;
-        u64 val;
-        struct cpufreq_ondemandplus_cpuinfo *pcpu =
-                &per_cpu(cpuinfo, smp_processor_id());
-
-        ret = strict_strtoull(buf, 0, &val);
-        if (ret < 0)
-                return ret;
-                
-        if (val > pcpu->policy->max)
-                val = pcpu->policy->max;
-
-        if (val < allowed_min)
-                val = allowed_min;
-
-        staycycles_resetfreq = val;
-        return count;
-}
-
-static struct global_attr staycycles_resetfreq_attr = __ATTR(staycycles_resetfreq, 0644,
-                show_staycycles_resetfreq, store_staycycles_resetfreq);
-
-static ssize_t show_io_is_busy(struct kobject *kobj,
-                        struct attribute *attr, char *buf)
-{
-        return sprintf(buf, "%u\n", io_is_busy);
-}
-
-static ssize_t store_io_is_busy(struct kobject *kobj,
-                        struct attribute *attr, const char *buf, size_t count)
-{
-        int ret;
-        unsigned long val;
-
-        ret = kstrtoul(buf, 0, &val);
-        if (ret < 0)
-                return ret;
-        io_is_busy = val;
-        return count;
-}
-
-static struct global_attr io_is_busy_attr = __ATTR(io_is_busy, 0644,
-                show_io_is_busy, store_io_is_busy);
-
-static struct attribute *ondemandplus_attributes[] = {
-        &timer_rate_attr.attr,
-        &up_threshold_attr.attr,
-        &down_differential_attr.attr,
-        &inter_hifreq_attr.attr,
-        &inter_lofreq_attr.attr,
-        &inter_staycycles_attr.attr,
-        &staycycles_resetfreq_attr.attr,
-        &io_is_busy_attr.attr,
-        NULL,
-};
-
-static struct attribute_group ondemandplus_attr_group = {
-        .attrs = ondemandplus_attributes,
-        .name = "ondemandplus",
-};
-
-static int cpufreq_ondemandplus_idle_notifier(struct notifier_block *nb,
-                                             unsigned long val,
-                                             void *data)
-{
-        switch (val) {
-        case IDLE_START:
-                cpufreq_ondemandplus_idle_start();
-                break;
-        case IDLE_END:
-                cpufreq_ondemandplus_idle_end();
-                break;
-        }
-
-        return 0;
-}
-
-static struct notifier_block cpufreq_ondemandplus_idle_nb = {
-        .notifier_call = cpufreq_ondemandplus_idle_notifier,
-};
-
-static int cpufreq_governor_ondemandplus(struct cpufreq_policy *policy,
-                unsigned int event)
-{
-        int rc;
-        unsigned int j;
-        struct cpufreq_ondemandplus_cpuinfo *pcpu;
-        struct cpufreq_frequency_table *freq_table;
-
-        switch (event) {
-        case CPUFREQ_GOV_START:
-                if (!cpu_online(policy->cpu))
-                        return -EINVAL;
-
-                freq_table =
-                        cpufreq_frequency_get_table(policy->cpu);
-
-                for_each_cpu(j, policy->cpus) {
-                        pcpu = &per_cpu(cpuinfo, j);
-                        pcpu->policy = policy;
-                        pcpu->target_freq = policy->cur;
-                        pcpu->freq_table = freq_table;
-                        pcpu->target_set_time_in_idle =
-                                get_cpu_idle_time(j,
-                                             &pcpu->target_set_time);
-                        pcpu->governor_enabled = 1;
-                        smp_wmb();
-                }
-
-                /*
-                 * Do not register the idle hook and create sysfs
-                 * entries if we have already done so.
-                 */
-                if (atomic_inc_return(&active_count) > 1)
-                        return 0;
-
-                rc = sysfs_create_group(cpufreq_global_kobject,
-                                &ondemandplus_attr_group);
-                if (rc)
-                        return rc;
-
-                idle_notifier_register(&cpufreq_ondemandplus_idle_nb);
-                break;
-
-        case CPUFREQ_GOV_STOP:
-                for_each_cpu(j, policy->cpus) {
-                        pcpu = &per_cpu(cpuinfo, j);
-                        pcpu->governor_enabled = 0;
-                        smp_wmb();
-                        del_timer_sync(&pcpu->cpu_timer);
-
-                        /*
-                         * Reset idle exit time since we may cancel the timer
-                         * before it can run after the last idle exit time,
-                         * to avoid tripping the check in idle exit for a timer
-                         * that is trying to run.
-                         */
-                        pcpu->idle_exit_time = 0;
-                }
-
-                if (atomic_dec_return(&active_count) > 0)
-                        return 0;
-
-                idle_notifier_unregister(&cpufreq_ondemandplus_idle_nb);
-                sysfs_remove_group(cpufreq_global_kobject,
-                                &ondemandplus_attr_group);
-
-                break;
-
-        case CPUFREQ_GOV_LIMITS:
-                if (policy->max < policy->cur)
-                        __cpufreq_driver_target(policy,
-                                        policy->max, CPUFREQ_RELATION_H);
-                else if (policy->min > policy->cur)
-                        __cpufreq_driver_target(policy,
-                                        policy->min, CPUFREQ_RELATION_L);
-                break;
-        }
-        return 0;
-}
-
-static int __init cpufreq_ondemandplus_init(void)
-{
-        unsigned int i;
-        struct cpufreq_ondemandplus_cpuinfo *pcpu;
-        struct sched_param param = { .sched_priority = MAX_RT_PRIO-1 };
-
-        timer_rate = DEFAULT_TIMER_RATE;
-        up_threshold = DEFAULT_UP_THRESHOLD;
-        down_differential = DEFAULT_DOWN_DIFFERENTIAL;
-        inter_hifreq = DEFAULT_INTER_HIFREQ;
-        allowed_min = DEFAULT_MIN_FREQ;
-        allowed_max = DEFAULT_MAX_FREQ;
-        suspend_frequency = SUSPEND_FREQ;
-        inter_lofreq = DEFAULT_INTER_LOFREQ;
-        inter_staycycles = DEFAULT_INTER_STAYCYCLES;
-        staycycles_resetfreq = DEFAULT_STAYCYCLES_RESETFREQ;
-        io_is_busy = DEFAULT_IO_IS_BUSY;
-
-        /* Initalize per-cpu timers */
-        for_each_possible_cpu(i) {
-                pcpu = &per_cpu(cpuinfo, i);
-                init_timer(&pcpu->cpu_timer);
-                pcpu->cpu_timer.function = cpufreq_ondemandplus_timer;
-                pcpu->cpu_timer.data = i;
-        }
-
-        spin_lock_init(&speedchange_cpumask_lock);
-        speedchange_task =
-                kthread_create(cpufreq_ondemandplus_speedchange_task, NULL,
-                                "cfondemandplus");
-        if (IS_ERR(speedchange_task))
-                return PTR_ERR(speedchange_task);
-
-        sched_setscheduler_nocheck(speedchange_task, SCHED_FIFO, &param);
-        get_task_struct(speedchange_task);
-
-        /* NB: wake up so the thread does not look hung to the freezer */
-        wake_up_process(speedchange_task);
-
-        return cpufreq_register_governor(&cpufreq_gov_ondemandplus);
-
-        put_task_struct(speedchange_task);
-        return -ENOMEM;
-}
-
-#ifdef CONFIG_CPU_FREQ_DEFAULT_GOV_ONDEMANDPLUS
-fs_initcall(cpufreq_ondemandplus_init);
-#else
-module_init(cpufreq_ondemandplus_init);
-#endif
-
-static void __exit cpufreq_ondemandplus_exit(void)
-{
-        cpufreq_unregister_governor(&cpufreq_gov_ondemandplus);
-        kthread_stop(speedchange_task);
-        put_task_struct(speedchange_task);
-}
-
-module_exit(cpufreq_ondemandplus_exit);
-
-MODULE_AUTHOR("Mike Chan <mike@android.com>");
-MODULE_DESCRIPTION("'cpufreq_ondemandplus' - A cpufreq governor for "
-        "semi-aggressive scaling");
-MODULE_LICENSE("GPL");
diff --git a/include/linux/cpufreq.h b/include/linux/cpufreq.h
index 4c46b98..6819de1 100644
--- a/include/linux/cpufreq.h
+++ b/include/linux/cpufreq.h
@@ -381,6 +381,9 @@ extern struct cpufreq_governor cpufreq_gov_ondemand;
 #elif defined(CONFIG_CPU_FREQ_DEFAULT_GOV_CONSERVATIVE)
 extern struct cpufreq_governor cpufreq_gov_conservative;
 #define CPUFREQ_DEFAULT_GOVERNOR	(&cpufreq_gov_conservative)
+#elif defined(CONFIG_CPU_FREQ_DEFAULT_GOV_ZZMOOVE)
+extern struct cpufreq_governor cpufreq_gov_zzmoove;
+#define CPUFREQ_DEFAULT_GOVERNOR       (&cpufreq_gov_zzmoove)
 #elif defined(CONFIG_CPU_FREQ_DEFAULT_GOV_INTERACTIVE)
 extern struct cpufreq_governor cpufreq_gov_interactive;
 #define CPUFREQ_DEFAULT_GOVERNOR	(&cpufreq_gov_interactive)
@@ -399,15 +402,9 @@ extern struct cpufreq_governor cpufreq_gov_lionheart;
 #elif defined(CONFIG_CPU_FREQ_DEFAULT_GOV_SMARTASSV2)
 extern struct cpufreq_governor cpufreq_gov_smartassv2;
 #define CPUFREQ_DEFAULT_GOVERNOR	(&cpufreq_gov_smartassv2)
-#elif defined(CONFIG_CPU_FREQ_DEFAULT_GOV_ABYSSPLUG)
-extern struct cpufreq_governor cpufreq_gov_abyssplug;
-#define CPUFREQ_DEFAULT_GOVERNOR (&cpufreq_gov_abyssplug)
 #elif defined(CONFIG_CPU_FREQ_DEFAULT_GOV_ABYSSPLUGV2)
 extern struct cpufreq_governor cpufreq_gov_abyssplugv2;
 #define CPUFREQ_DEFAULT_GOVERNOR (&cpufreq_gov_abyssplugv2)
-#elif defined(CONFIG_CPU_FREQ_DEFAULT_GOV_ONDEMANDPLUS)
-extern struct cpufreq_governor cpufreq_gov_ondemandplus;
-#define CPUFREQ_DEFAULT_GOVERNOR (&cpufreq_gov_ondemandplus)
 #elif defined(CONFIG_CPU_FREQ_DEFAULT_GOV_INTELLIMM)
 extern struct cpufreq_governor cpufreq_gov_intellimm;
 #define CPUFREQ_DEFAULT_GOVERNOR (&cpufreq_gov_intellimm)
--
libgit2 0.22.0

